{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# Gensim libraries:\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import LdaModel, CoherenceModel, LdaMulticore\n",
    "import pyLDAvis\n",
    "from pyLDAvis import gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "from pprint import pprint\n",
    "\n",
    "# NLTK:\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "\n",
    "# Matplotlib:\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Word-cloud:\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Read the dataset:\n",
    "Here we read the dataset that we have annotated with Vader, nrc and moralstrength. The idea is to add the topic scores besides the other lyrics features scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31729, 28)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>title</th>\n",
       "      <th>original_lyrics</th>\n",
       "      <th>cleaned_lyrics</th>\n",
       "      <th>lang_detect_spacy</th>\n",
       "      <th>vader_neg</th>\n",
       "      <th>vader_neu</th>\n",
       "      <th>vader_pos</th>\n",
       "      <th>vader_comp</th>\n",
       "      <th>words</th>\n",
       "      <th>...</th>\n",
       "      <th>sadness</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>surprise</th>\n",
       "      <th>joy</th>\n",
       "      <th>trust</th>\n",
       "      <th>care</th>\n",
       "      <th>fairness</th>\n",
       "      <th>loyalty</th>\n",
       "      <th>authority</th>\n",
       "      <th>purity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>*NSYNC</td>\n",
       "      <td>Bye Bye Bye</td>\n",
       "      <td>[Intro: Justin &amp; All]\\nHey, hey\\nBye bye bye\\n...</td>\n",
       "      <td>Hey, hey Bye bye bye Bye bye! Bye bye!    I'm ...</td>\n",
       "      <td>en</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.9887</td>\n",
       "      <td>['hey', 'hey', 'bye', 'bye', 'bye', 'bye', 'by...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170940</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.017094</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.166667</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>*NSYNC</td>\n",
       "      <td>It’s Gonna Be Me</td>\n",
       "      <td>[Intro: Justin]\\n(It's gonna be me)\\nOooh yeah...</td>\n",
       "      <td>Oooh yeah   You might've been hurt, babe That ...</td>\n",
       "      <td>en</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.9887</td>\n",
       "      <td>['oooh', 'yeah', 'you', 'might', 've', 'been',...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086022</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.150538</td>\n",
       "      <td>0.053763</td>\n",
       "      <td>2.285714</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>*NSYNC</td>\n",
       "      <td>Tearin’ Up My Heart</td>\n",
       "      <td>[Chorus: JC &amp; Justin]\\nIt's tearin' up my hear...</td>\n",
       "      <td>It's tearin' up my heart when I'm with you But...</td>\n",
       "      <td>en</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.9927</td>\n",
       "      <td>['it', 's', 'tearin', 'up', 'my', 'heart', 'wh...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070588</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>*NSYNC</td>\n",
       "      <td>Gone</td>\n",
       "      <td>[Verse 1: Justin]\\nThere's a thousand words th...</td>\n",
       "      <td>There's a thousand words that I could say To m...</td>\n",
       "      <td>en</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.9904</td>\n",
       "      <td>['there', 's', 'a', 'thousand', 'words', 'that...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012903</td>\n",
       "      <td>0.038710</td>\n",
       "      <td>0.045161</td>\n",
       "      <td>0.077419</td>\n",
       "      <td>0.070968</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.166667</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>*NSYNC</td>\n",
       "      <td>Merry Christmas, Happy Holidays</td>\n",
       "      <td>[Intro: Justin, All &amp; JC]\\nOooh, ooh ooh\\nMerr...</td>\n",
       "      <td>Oooh, ooh ooh Merry Christmas Happy holidays M...</td>\n",
       "      <td>en</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>['oooh', 'ooh', 'ooh', 'merry', 'christmas', '...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036697</td>\n",
       "      <td>0.348624</td>\n",
       "      <td>0.137615</td>\n",
       "      <td>0.302752</td>\n",
       "      <td>0.192661</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.166667</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Artist                            title  \\\n",
       "0  *NSYNC                      Bye Bye Bye   \n",
       "1  *NSYNC                 It’s Gonna Be Me   \n",
       "2  *NSYNC              Tearin’ Up My Heart   \n",
       "3  *NSYNC                             Gone   \n",
       "4  *NSYNC  Merry Christmas, Happy Holidays   \n",
       "\n",
       "                                     original_lyrics  \\\n",
       "0  [Intro: Justin & All]\\nHey, hey\\nBye bye bye\\n...   \n",
       "1  [Intro: Justin]\\n(It's gonna be me)\\nOooh yeah...   \n",
       "2  [Chorus: JC & Justin]\\nIt's tearin' up my hear...   \n",
       "3  [Verse 1: Justin]\\nThere's a thousand words th...   \n",
       "4  [Intro: Justin, All & JC]\\nOooh, ooh ooh\\nMerr...   \n",
       "\n",
       "                                      cleaned_lyrics lang_detect_spacy  \\\n",
       "0  Hey, hey Bye bye bye Bye bye! Bye bye!    I'm ...                en   \n",
       "1  Oooh yeah   You might've been hurt, babe That ...                en   \n",
       "2  It's tearin' up my heart when I'm with you But...                en   \n",
       "3  There's a thousand words that I could say To m...                en   \n",
       "4  Oooh, ooh ooh Merry Christmas Happy holidays M...                en   \n",
       "\n",
       "   vader_neg  vader_neu  vader_pos  vader_comp  \\\n",
       "0      0.083      0.746      0.171      0.9887   \n",
       "1      0.083      0.728      0.189      0.9887   \n",
       "2      0.180      0.747      0.073     -0.9927   \n",
       "3      0.076      0.772      0.152      0.9904   \n",
       "4      0.014      0.478      0.508      0.9998   \n",
       "\n",
       "                                               words  ...   sadness  \\\n",
       "0  ['hey', 'hey', 'bye', 'bye', 'bye', 'bye', 'by...  ...  0.170940   \n",
       "1  ['oooh', 'yeah', 'you', 'might', 've', 'been',...  ...  0.086022   \n",
       "2  ['it', 's', 'tearin', 'up', 'my', 'heart', 'wh...  ...  0.082353   \n",
       "3  ['there', 's', 'a', 'thousand', 'words', 'that...  ...  0.012903   \n",
       "4  ['oooh', 'ooh', 'ooh', 'merry', 'christmas', '...  ...  0.036697   \n",
       "\n",
       "  anticipation  surprise       joy     trust      care  fairness  loyalty  \\\n",
       "0     0.025641  0.025641  0.102564  0.017094  4.000000  8.166667      5.0   \n",
       "1     0.064516  0.161290  0.150538  0.053763  2.285714  5.000000      5.0   \n",
       "2     0.023529  0.000000  0.070588  0.011765  5.000000  5.000000      5.0   \n",
       "3     0.038710  0.045161  0.077419  0.070968  5.000000  8.166667      5.0   \n",
       "4     0.348624  0.137615  0.302752  0.192661  5.000000  8.166667      8.0   \n",
       "\n",
       "   authority  purity  \n",
       "0        5.0     8.0  \n",
       "1        5.0     5.0  \n",
       "2        5.0     5.0  \n",
       "3        5.0     8.0  \n",
       "4        5.0     8.0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_lyrics_sent_emo_morals_dt = pd.read_csv('../data/artist_lyrics_annotated_vader_nrc_moralStrength.csv')\n",
    "print(en_lyrics_sent_emo_morals_dt.shape)\n",
    "en_lyrics_sent_emo_morals_dt.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing Step:\n",
    "We skip preprocessing step and just read the lemmas we saved from when we extracted other lyrics features (see point 6 in *lyrics_vader_nrc_moral_strength_annotations.ipynb*). Instead, we just read the lemmas of each song from the json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/lyrics_lemmas') as f:\n",
    "    lyrics_spacy_lemmas = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31729"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lyrics_spacy_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. LDA Dictionary Creation and occurrence-based Filtering:\n",
    "To perform Latent Dirichlet Allocation, the well-established Python library-gensim is used, which requires a dictionary representation of the documents. Meaning that all tokens are mapped to a unique ID, this reduces the overall dimensionality of a literature corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(lyrics_spacy_lemmas)\n",
    "dictionary.filter_extremes(no_above = 0.9) #no_below = 340,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Bag-of-Words and Index to Dictionary Conversion:\n",
    "Each song (transformed in a list of tokens) is converted into the bag-of-words, which stores the unique token ID and its count for each song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 13902\n",
      "Number of documents: 31729\n",
      "[[(0, 1), (1, 9), (2, 1), (3, 3), (4, 3), (5, 2), (6, 1), (7, 5), (8, 1), (9, 5), (10, 4), (11, 1), (12, 2), (13, 1), (14, 1), (15, 1), (16, 2), (17, 1), (18, 4), (19, 2), (20, 9), (21, 1), (22, 1), (23, 1), (24, 2), (25, 5), (26, 1), (27, 3), (28, 1), (29, 4), (30, 2), (31, 1), (32, 3), (33, 1), (34, 3), (35, 1), (36, 1), (37, 2), (38, 3), (39, 1), (40, 1), (41, 4), (42, 1), (43, 12), (44, 1), (45, 1), (46, 1)]]\n"
     ]
    }
   ],
   "source": [
    "gensim_corpus = [dictionary.doc2bow(song) for song in lyrics_spacy_lemmas]\n",
    "temp = dictionary[0]\n",
    "# id2word = dictionary.id2token\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(gensim_corpus))\n",
    "print(gensim_corpus[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Choosing the number of topics in LDA:\n",
    "Here we find the optimal number of topics. We need to build many LDA models with different values of the number of topics (k) and pick the one that gives the highest coherence value. Choosing a ‘k’ that marks the end of a rapid growth of topic coherence usually offers meaningful and interpretable topics. Picking an even higher value can sometimes provide more granular sub-topics. If you see the same keywords being repeated in multiple topics, it’s probably a sign that the ‘k’ is too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_and_perplexity_values(dictionary, gensim_corpus,\n",
    "                             lyrics_spacy_lemmas, limit, start=2, step=2):\n",
    "    coherence_values = []\n",
    "    u_mass_values = []\n",
    "    perplexity_values = []\n",
    "    model_list = []\n",
    "    \n",
    "    for num_topics in range(start, limit, step):\n",
    "        model=LdaModel(corpus=gensim_corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        model_list.append(model)\n",
    "        \n",
    "        # C_V:\n",
    "        coherencemodel = CoherenceModel(model=model, texts=lyrics_spacy_lemmas, \n",
    "                                        dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "        \n",
    "        # U_mass:\n",
    "        coherencemodel_u_mass = CoherenceModel(model=model, texts=lyrics_spacy_lemmas, \n",
    "                                               dictionary=dictionary, coherence=\"u_mass\")\n",
    "        u_mass_values .append(coherencemodel_u_mass.get_coherence())\n",
    "        \n",
    "        # Perplexity:\n",
    "        perplexitymodel = model.log_perplexity(gensim_corpus)\n",
    "        perplexity_values.append(perplexitymodel)\n",
    "\n",
    "\n",
    "        \n",
    "    return model_list, coherence_values, u_mass_values, perplexity_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=2;limit=20; step=2\n",
    "\n",
    "model_list, coherence_values, u_mass_values, perplexity_values  = compute_coherence_and_perplexity_values(dictionary=dictionary, \n",
    "                            gensim_corpus=gensim_corpus, \n",
    "                            lyrics_spacy_lemmas=lyrics_spacy_lemmas, \n",
    "                            start=start, limit=limit, step=step)\n",
    "\n",
    "x = range(start, limit, step)\n",
    "\n",
    "# Show c_v graph\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()\n",
    "\n",
    "# Show u_mass graph\n",
    "plt.plot(x, u_mass_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score U_mass\")\n",
    "plt.legend((\"u_mass_values\"), loc='best')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Show perplexity graph\n",
    "plt.plot(x, perplexity_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Perplexity score\")\n",
    "plt.legend((\"perplexity_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*4.1. The LDA model with the proposed number of k topics based on the coherence and perplexity estimations:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 100\n",
    "passes = 10\n",
    "iterations = 100\n",
    "num_topics = 4\n",
    "random_state = 100\n",
    "\n",
    "lda_model = LdaModel(corpus=gensim_corpus,\n",
    "                    id2word=dictionary,\n",
    "                    chunksize=chunksize,\n",
    "#                     alpha='auto',\n",
    "#                     eta='auto',\n",
    "                    iterations=iterations,\n",
    "                    num_topics=num_topics,\n",
    "                    passes=passes,\n",
    "                    per_word_topics = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.050*\"get\" + 0.012*\"go\" + 0.011*\"man\" + 0.011*\"girl\" + 0.009*\"say\" + '\n",
      "  '0.009*\"fuck\" + 0.009*\"come\" + 0.008*\"back\" + 0.008*\"make\" + 0.007*\"bitch\"'),\n",
      " (1,\n",
      "  '0.019*\"come\" + 0.015*\"light\" + 0.014*\"night\" + 0.014*\"see\" + 0.012*\"eye\" + '\n",
      "  '0.012*\"fall\" + 0.012*\"away\" + 0.010*\"go\" + 0.009*\"run\" + 0.009*\"day\"'),\n",
      " (2,\n",
      "  '0.013*\"die\" + 0.011*\"life\" + 0.009*\"man\" + 0.009*\"blood\" + 0.008*\"dead\" + '\n",
      "  '0.008*\"live\" + 0.008*\"fight\" + 0.007*\"world\" + 0.007*\"death\" + '\n",
      "  '0.006*\"kill\"'),\n",
      " (3,\n",
      "  '0.040*\"love\" + 0.038*\"know\" + 0.024*\"go\" + 0.020*\"say\" + 0.020*\"get\" + '\n",
      "  '0.018*\"make\" + 0.018*\"want\" + 0.017*\"never\" + 0.017*\"time\" + 0.016*\"feel\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4243921924737374\n"
     ]
    }
   ],
   "source": [
    "coherencemodel = CoherenceModel(model=lda_model, texts=lyrics_spacy_lemmas, dictionary=dictionary, coherence='c_v')\n",
    "print(coherencemodel.get_coherence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We decided to keep 4 general topics for the song lyrics:\n",
    "Based on the Coherence and Perplexity estimations 4-6 topics are the most suited for the song lyrics in our dataset. We tried multiple combinations and the most clear definition of the topis was for k=4. Feel free to try other k values (6, 8, etc).\n",
    "\n",
    "We named our 4 topics as follows: \n",
    "**1. World/Life/Time:** We named this topic like this since it contains keywords as light, night, day, time, world, etc,that are related to living and have more neutral context; \n",
    "**2. Obscene:** This topic contains swearing and insulting words;\n",
    "**3. Romantic:** This topic contains words related to love;\n",
    "**Death/Fear/Violence:** This topic contains words related to war, death and darkness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Save the LDA model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../data/lda_model_best_k4.pk\",\"wb\") as f:\n",
    "#     pickle.dump(lda_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*5.1.Read the best LDA model that we hace saved:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/lda_model_best_k4.pk', 'rb') as pickle_file:\n",
    "    lda_model= pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.050*\"get\" + 0.012*\"go\" + 0.011*\"man\" + 0.011*\"girl\" + 0.009*\"say\" + '\n",
      "  '0.009*\"fuck\" + 0.009*\"come\" + 0.008*\"back\" + 0.008*\"make\" + 0.007*\"bitch\"'),\n",
      " (1,\n",
      "  '0.019*\"come\" + 0.015*\"light\" + 0.014*\"night\" + 0.014*\"see\" + 0.012*\"eye\" + '\n",
      "  '0.012*\"fall\" + 0.012*\"away\" + 0.010*\"go\" + 0.009*\"run\" + 0.009*\"day\"'),\n",
      " (2,\n",
      "  '0.013*\"die\" + 0.011*\"life\" + 0.009*\"man\" + 0.009*\"blood\" + 0.008*\"dead\" + '\n",
      "  '0.008*\"live\" + 0.008*\"fight\" + 0.007*\"world\" + 0.007*\"death\" + '\n",
      "  '0.006*\"kill\"'),\n",
      " (3,\n",
      "  '0.040*\"love\" + 0.038*\"know\" + 0.024*\"go\" + 0.020*\"say\" + 0.020*\"get\" + '\n",
      "  '0.018*\"make\" + 0.018*\"want\" + 0.017*\"never\" + 0.017*\"time\" + 0.016*\"feel\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Visualise the LDA model and save as .html file using pyLDAvis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data = gensimvis.prepare(lda_model, gensim_corpus, dictionary)\n",
    "#pyLDAvis.display(vis_data)\n",
    "pyLDAvis.save_html(vis_data, '../data/./lyrics_lda_7_k_'+ str(num_topics) +'.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. The Dominant topic and its percentage contribution in each Lyrics\n",
    "In LDA models, each document is composed of multiple topics. But, typically only one of the topics is dominant. The code below extracts the dominant topic for each lyrics and shows the weight of the topic and the keywords in a nicely formatted output. This way, we will know which lyrics belongs predominantly to which topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_lyrics(ldamodel=None, corpus=gensim_corpus, texts=lyrics_spacy_lemmas):    \n",
    "    lyrics_topics_dt = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each lyrics\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list # assign weights per each lyrics rows          \n",
    "      \n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True) #sorted rows based on the weights\n",
    "        \n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row): # topic num and propability\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num) # ('word', topic_pribablity)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp]) # join keywords per topic\n",
    "                lyrics_topics_dt = lyrics_topics_dt.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    lyrics_topics_dt.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    lyrics_topics_dt = pd.concat([lyrics_topics_dt, contents], axis=1) #column wise\n",
    "    return(lyrics_topics_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_lyrics_keywords_dt = format_topics_lyrics(ldamodel = lda_model, \n",
    "                                                corpus = gensim_corpus, \n",
    "                                                texts = lyrics_spacy_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Perc_Contribution</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.8785</td>\n",
       "      <td>love, know, go, say, get, make, want, never, t...</td>\n",
       "      <td>[tonight, probably, going, start, fight, know,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9537</td>\n",
       "      <td>love, know, go, say, get, make, want, never, t...</td>\n",
       "      <td>[hurt, babe, lie, see, come, go, remember, tel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9905</td>\n",
       "      <td>love, know, go, say, get, make, want, never, t...</td>\n",
       "      <td>[tearin, heart, apart, feel, matter, feel, pai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9337</td>\n",
       "      <td>love, know, go, say, get, make, want, never, t...</td>\n",
       "      <td>[word, say, make, come, home, seem, long, ago,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.6957</td>\n",
       "      <td>love, know, go, say, get, make, want, never, t...</td>\n",
       "      <td>[wait, year, night, snow, glisten, tree, outsi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31724</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8494</td>\n",
       "      <td>come, light, night, see, eye, fall, away, go, ...</td>\n",
       "      <td>[sun, old, water, yearle, flake, keep, whirl, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31725</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5175</td>\n",
       "      <td>love, know, go, say, get, make, want, never, t...</td>\n",
       "      <td>[moon, star, feel, arrest, unknowable, fade, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31726</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5566</td>\n",
       "      <td>love, know, go, say, get, make, want, never, t...</td>\n",
       "      <td>[get, big, big, distance, outside, world, beco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31727</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8014</td>\n",
       "      <td>come, light, night, see, eye, fall, away, go, ...</td>\n",
       "      <td>[come, come, warn, wake, wake, call, back, ope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31728</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6992</td>\n",
       "      <td>come, light, night, see, eye, fall, away, go, ...</td>\n",
       "      <td>[arm, river, sinuous, hill, woe, wind, wear, s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31729 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Dominant_Topic  Perc_Contribution  \\\n",
       "0                 3.0             0.8785   \n",
       "1                 3.0             0.9537   \n",
       "2                 3.0             0.9905   \n",
       "3                 3.0             0.9337   \n",
       "4                 3.0             0.6957   \n",
       "...               ...                ...   \n",
       "31724             1.0             0.8494   \n",
       "31725             3.0             0.5175   \n",
       "31726             3.0             0.5566   \n",
       "31727             1.0             0.8014   \n",
       "31728             1.0             0.6992   \n",
       "\n",
       "                                          Topic_Keywords  \\\n",
       "0      love, know, go, say, get, make, want, never, t...   \n",
       "1      love, know, go, say, get, make, want, never, t...   \n",
       "2      love, know, go, say, get, make, want, never, t...   \n",
       "3      love, know, go, say, get, make, want, never, t...   \n",
       "4      love, know, go, say, get, make, want, never, t...   \n",
       "...                                                  ...   \n",
       "31724  come, light, night, see, eye, fall, away, go, ...   \n",
       "31725  love, know, go, say, get, make, want, never, t...   \n",
       "31726  love, know, go, say, get, make, want, never, t...   \n",
       "31727  come, light, night, see, eye, fall, away, go, ...   \n",
       "31728  come, light, night, see, eye, fall, away, go, ...   \n",
       "\n",
       "                                                       0  \n",
       "0      [tonight, probably, going, start, fight, know,...  \n",
       "1      [hurt, babe, lie, see, come, go, remember, tel...  \n",
       "2      [tearin, heart, apart, feel, matter, feel, pai...  \n",
       "3      [word, say, make, come, home, seem, long, ago,...  \n",
       "4      [wait, year, night, snow, glisten, tree, outsi...  \n",
       "...                                                  ...  \n",
       "31724  [sun, old, water, yearle, flake, keep, whirl, ...  \n",
       "31725  [moon, star, feel, arrest, unknowable, fade, d...  \n",
       "31726  [get, big, big, distance, outside, world, beco...  \n",
       "31727  [come, come, warn, wake, wake, call, back, ope...  \n",
       "31728  [arm, river, sinuous, hill, woe, wind, wear, s...  \n",
       "\n",
       "[31729 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_lyrics_keywords_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*7.1. Format dominant keyword dataframe*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.8785</td>\n",
       "      <td>love, know, go, say, get, make, want, never, t...</td>\n",
       "      <td>[tonight, probably, going, start, fight, know,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9537</td>\n",
       "      <td>love, know, go, say, get, make, want, never, t...</td>\n",
       "      <td>[hurt, babe, lie, see, come, go, remember, tel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9905</td>\n",
       "      <td>love, know, go, say, get, make, want, never, t...</td>\n",
       "      <td>[tearin, heart, apart, feel, matter, feel, pai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9337</td>\n",
       "      <td>love, know, go, say, get, make, want, never, t...</td>\n",
       "      <td>[word, say, make, come, home, seem, long, ago,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.6957</td>\n",
       "      <td>love, know, go, say, get, make, want, never, t...</td>\n",
       "      <td>[wait, year, night, snow, glisten, tree, outsi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.7666</td>\n",
       "      <td>love, know, go, say, get, make, want, never, t...</td>\n",
       "      <td>[hang, see, new, boyfriend, make, jealous, hop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.8800</td>\n",
       "      <td>love, know, go, say, get, make, want, never, t...</td>\n",
       "      <td>[vision, bring, tear, eye, surround, secret, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5928</td>\n",
       "      <td>get, go, man, girl, say, fuck, come, back, mak...</td>\n",
       "      <td>[dirty, pop, sick, tired, people, talk, deal, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9901</td>\n",
       "      <td>love, know, go, say, get, make, want, never, t...</td>\n",
       "      <td>[ever, want, ever, need, tell, lance, want, ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4777</td>\n",
       "      <td>come, light, night, see, eye, fall, away, go, ...</td>\n",
       "      <td>[cold, salty, sea, water, touch, lip, cold, sa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0            0             3.0              0.8785   \n",
       "1            1             3.0              0.9537   \n",
       "2            2             3.0              0.9905   \n",
       "3            3             3.0              0.9337   \n",
       "4            4             3.0              0.6957   \n",
       "5            5             3.0              0.7666   \n",
       "6            6             3.0              0.8800   \n",
       "7            7             0.0              0.5928   \n",
       "8            8             3.0              0.9901   \n",
       "9            9             1.0              0.4777   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  love, know, go, say, get, make, want, never, t...   \n",
       "1  love, know, go, say, get, make, want, never, t...   \n",
       "2  love, know, go, say, get, make, want, never, t...   \n",
       "3  love, know, go, say, get, make, want, never, t...   \n",
       "4  love, know, go, say, get, make, want, never, t...   \n",
       "5  love, know, go, say, get, make, want, never, t...   \n",
       "6  love, know, go, say, get, make, want, never, t...   \n",
       "7  get, go, man, girl, say, fuck, come, back, mak...   \n",
       "8  love, know, go, say, get, make, want, never, t...   \n",
       "9  come, light, night, see, eye, fall, away, go, ...   \n",
       "\n",
       "                                              Lyrics  \n",
       "0  [tonight, probably, going, start, fight, know,...  \n",
       "1  [hurt, babe, lie, see, come, go, remember, tel...  \n",
       "2  [tearin, heart, apart, feel, matter, feel, pai...  \n",
       "3  [word, say, make, come, home, seem, long, ago,...  \n",
       "4  [wait, year, night, snow, glisten, tree, outsi...  \n",
       "5  [hang, see, new, boyfriend, make, jealous, hop...  \n",
       "6  [vision, bring, tear, eye, surround, secret, l...  \n",
       "7  [dirty, pop, sick, tired, people, talk, deal, ...  \n",
       "8  [ever, want, ever, need, tell, lance, want, ba...  \n",
       "9  [cold, salty, sea, water, touch, lip, cold, sa...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dominant_topic_dt = topic_lyrics_keywords_dt.reset_index()\n",
    "dominant_topic_dt.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Lyrics']\n",
    "dominant_topic_dt.reset_index(drop = True, inplace = True)\n",
    "dominant_topic_dt[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0    15340\n",
       "1.0     7140\n",
       "0.0     5700\n",
       "2.0     3549\n",
       "Name: Dominant_Topic, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dominant_topic_dt.Dominant_Topic.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*7.2. Let's add the topics to the rest of lyrics emotions and sentiment dataframe:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dominant_topic_dt['Dominant_Topic'] = dominant_topic_dt['Dominant_Topic'].replace(2.0, 'Death/Fear/Violence')\n",
    "dominant_topic_dt['Dominant_Topic'] = dominant_topic_dt['Dominant_Topic'].replace(1.0, 'World/Life/Time')\n",
    "dominant_topic_dt['Dominant_Topic'] = dominant_topic_dt['Dominant_Topic'].replace(3.0, 'Romantic')\n",
    "dominant_topic_dt['Dominant_Topic'] = dominant_topic_dt['Dominant_Topic'].replace(0.0, 'Obscene')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Concatenate the artist lyrics features dataset with the extracted topics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_lyrics_sent_emo_morals_topics_dt = pd.concat([en_lyrics_sent_emo_morals_dt,\n",
    "                                                         dominant_topic_dt[['Dominant_Topic','Topic_Perc_Contrib']]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>title</th>\n",
       "      <th>original_lyrics</th>\n",
       "      <th>cleaned_lyrics</th>\n",
       "      <th>lang_detect_spacy</th>\n",
       "      <th>vader_neg</th>\n",
       "      <th>vader_neu</th>\n",
       "      <th>vader_pos</th>\n",
       "      <th>vader_comp</th>\n",
       "      <th>words</th>\n",
       "      <th>...</th>\n",
       "      <th>surprise</th>\n",
       "      <th>joy</th>\n",
       "      <th>trust</th>\n",
       "      <th>care</th>\n",
       "      <th>fairness</th>\n",
       "      <th>loyalty</th>\n",
       "      <th>authority</th>\n",
       "      <th>purity</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>*NSYNC</td>\n",
       "      <td>Bye Bye Bye</td>\n",
       "      <td>[Intro: Justin &amp; All]\\nHey, hey\\nBye bye bye\\n...</td>\n",
       "      <td>Hey, hey Bye bye bye Bye bye! Bye bye!    I'm ...</td>\n",
       "      <td>en</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.9887</td>\n",
       "      <td>['hey', 'hey', 'bye', 'bye', 'bye', 'bye', 'by...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.017094</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.166667</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Romantic</td>\n",
       "      <td>0.8785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>*NSYNC</td>\n",
       "      <td>It’s Gonna Be Me</td>\n",
       "      <td>[Intro: Justin]\\n(It's gonna be me)\\nOooh yeah...</td>\n",
       "      <td>Oooh yeah   You might've been hurt, babe That ...</td>\n",
       "      <td>en</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.9887</td>\n",
       "      <td>['oooh', 'yeah', 'you', 'might', 've', 'been',...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.150538</td>\n",
       "      <td>0.053763</td>\n",
       "      <td>2.285714</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Romantic</td>\n",
       "      <td>0.9537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>*NSYNC</td>\n",
       "      <td>Tearin’ Up My Heart</td>\n",
       "      <td>[Chorus: JC &amp; Justin]\\nIt's tearin' up my hear...</td>\n",
       "      <td>It's tearin' up my heart when I'm with you But...</td>\n",
       "      <td>en</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.9927</td>\n",
       "      <td>['it', 's', 'tearin', 'up', 'my', 'heart', 'wh...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070588</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Romantic</td>\n",
       "      <td>0.9905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Artist                title  \\\n",
       "0  *NSYNC          Bye Bye Bye   \n",
       "1  *NSYNC     It’s Gonna Be Me   \n",
       "2  *NSYNC  Tearin’ Up My Heart   \n",
       "\n",
       "                                     original_lyrics  \\\n",
       "0  [Intro: Justin & All]\\nHey, hey\\nBye bye bye\\n...   \n",
       "1  [Intro: Justin]\\n(It's gonna be me)\\nOooh yeah...   \n",
       "2  [Chorus: JC & Justin]\\nIt's tearin' up my hear...   \n",
       "\n",
       "                                      cleaned_lyrics lang_detect_spacy  \\\n",
       "0  Hey, hey Bye bye bye Bye bye! Bye bye!    I'm ...                en   \n",
       "1  Oooh yeah   You might've been hurt, babe That ...                en   \n",
       "2  It's tearin' up my heart when I'm with you But...                en   \n",
       "\n",
       "   vader_neg  vader_neu  vader_pos  vader_comp  \\\n",
       "0      0.083      0.746      0.171      0.9887   \n",
       "1      0.083      0.728      0.189      0.9887   \n",
       "2      0.180      0.747      0.073     -0.9927   \n",
       "\n",
       "                                               words  ...  surprise       joy  \\\n",
       "0  ['hey', 'hey', 'bye', 'bye', 'bye', 'bye', 'by...  ...  0.025641  0.102564   \n",
       "1  ['oooh', 'yeah', 'you', 'might', 've', 'been',...  ...  0.161290  0.150538   \n",
       "2  ['it', 's', 'tearin', 'up', 'my', 'heart', 'wh...  ...  0.000000  0.070588   \n",
       "\n",
       "      trust      care  fairness  loyalty  authority  purity  Dominant_Topic  \\\n",
       "0  0.017094  4.000000  8.166667      5.0        5.0     8.0        Romantic   \n",
       "1  0.053763  2.285714  5.000000      5.0        5.0     5.0        Romantic   \n",
       "2  0.011765  5.000000  5.000000      5.0        5.0     5.0        Romantic   \n",
       "\n",
       "   Topic_Perc_Contrib  \n",
       "0              0.8785  \n",
       "1              0.9537  \n",
       "2              0.9905  \n",
       "\n",
       "[3 rows x 30 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_lyrics_sent_emo_morals_topics_dt.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Save the dataset with all lyrics features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_lyrics_sent_emo_morals_topics_dt.to_csv('../data/artist_lyrics_annotated_vader_nrc_moralStrength_lda.csv', \n",
    "                                           index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. References:\n",
    "1. Temporal Analysis and Visualisation of Music (Misael et al 2020): https://sol.sbc.org.br/index.php/eniac/article/view/12155\n",
    "2. Evaluate Topic Models: Latent Dirichlet Allocation (LDA): https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0\n",
    "3. What’s in a Song? LDA Topic Modeling of over 120,000 Lyrics:\n",
    "https://tim-denzler.medium.com/whats-in-a-song-using-lda-to-find-topics-in-over-120-000-songs-53785767b692\n",
    "4. Evaluation of Topic Modeling: Topic Coherence: https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
